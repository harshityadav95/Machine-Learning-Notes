# AI-900T00-A

## About this Course <a id="1023959"></a>

Welcome to this course on Azure AI Fundamentals!

This course is designed for anyone who wants to learn about artificial intelligence \(AI\) and the services in Microsoft Azure that you can use to build AI solutions. The course provides a practical, hands-on approach in which you will get a chance to see AI in action and try Azure AI services for yourself.

The materials in this workbook are designed to be used alongside online modules in [Microsoft Learn](https://docs.microsoft.com/learn). Throughout the course, you'll find references to specific Learn modules that you should use to supplement the information here.

**Learning objectives**

After completing this course, you will be able to:

* Describe Artificial Intelligence workloads and considerations.
* Describe fundamental principles of machine learning on Azure.
* Describe features of computer vision workloads on Azure.
* Describe features of Natural Language Processing \(NLP\) workloads on Azure.
* Describe features of conversational AI workloads on Azure.

## Course Agenda <a id="1023960"></a>

This course includes the following modules:

**Module 1: Introduction to AI**

In this module, you'll learn about common uses of artificial intelligence \(AI\), and the different types of workload associated with AI. You'll then explore considerations and principles for responsible AI development.

**Module 2: Machine Learning**

Machine learning is the foundation for modern AI solutions. In this module, you'll learn about some fundamental machine learning concepts, and how to use the Azure Machine Learning service to create and publish machine learning models.

**Module 3: Computer Vision**

Computer vision is a the area of AI that deals with understanding the world visually, through images, video files, and cameras. In this module you'll explore multiple computer vision techniques and services.

**Module 4: Natural Language Processing**

This module describes scenarios for AI solutions that can process written and spoken language. You'll learn about Azure services that can be used to build solutions that analyze text, recognize and synthesize speech, translate between languages, and interpret commands.

**Module 5: Conversational AI**

Conversational AI enables users to engage in a dialog with an AI agent, or bot, through communication channels such as email, webchat interfaces, social media, and others. This module describes some basic principles for working with bots and gives you an opportunity to create a bot that can respond intelligently to user questions.

## What is Artificial Intelligence? <a id="1023964"></a>

Simply put, AI is the creation of software that imitates human behaviors and capabilities. Key elements include:

* Making decisions based on data and past experience
* Detecting anomalies
* Interpreting visual input
* Understanding written and spoken language
* Engaging in dialogs and conversations

## Common Artificial Intelligence Workloads <a id="1023965"></a>

Common AI-related workloads include:

* **Machine learning** - This is often the foundation for an AI system, and is the way we “teach” a computer model to make prediction and draw conclusions from data.
* **Anomaly detection** - The capability to automatically detect errors or unusual activity in a system.
* **Computer vision** - The capability of software to interpret the world visually through cameras, video, and images.
* **Natural language processing** - The capability for a computer to interpret written or spoken language, and respond in kind.
* **Conversational AI** - The capability of a software agent \(usually referred to as a bot\) to participate in a conversation.

## Artificial Intelligence in Microsoft Azure <a id="1023966"></a>

Microsoft Azure provides a scalable, reliable cloud platform for AI, including:

* Data storage
* Compute
* Services

Some of the key AI-related services in Azure are described in this table:

| Service | Description |
| :--- | :--- |
| Azure Machine Learning | A platform for training, deploying, and managing machine learning models |
| Cognitive Services | A suite of services developers can use to build AI solutions |
| Azure Bot Service | A cloud-based platform for developing and managing bots |

## Challenges and Risks with AI <a id="1023968"></a>

Artificial Intelligence is a powerful tool that can be used to greatly benefit the world. However, like any tool, it must be used responsibly.

The following table shows some of the potential challenges risks facing an AI application developer.

| Challenge or Risk | Example |
| :--- | :--- |
| Bias can affect results | A loan-approval model discriminates by gender due to bias in the data with which it was trained |
| Errors may cause harm | An autonomous vehicle experiences a system failure and causes a collision |
| Data could be exposed | A medical diagnostic bot is trained using sensitive patient data, which is stored insecurely |
| Solutions may not work for everyone | A home automation assistant provides no audio output for visually impaired users |
| Users must trust a complex system | An AI-based financial tool makes investment recommendations - what are they based on? |
| Who's liable for AI-driven decisions? | An innocent person is convicted of a crime based on evidence from facial recognition – who's responsible? |

## Principles of Responsible AI <a id="1023969"></a>

At Microsoft, AI software development is guided by a set of six principles, designed to ensure that AI applications provide amazing solutions to difficult problems without any unintended negative consequences.

**Fairness**

AI systems should treat all people fairly. For example, suppose you create a machine learning model to support a loan approval application for a bank. The model should make predictions of whether or not the loan should be approved without incorporating any bias based on gender, ethnicity, or other factors that might result in an unfair advantage or disadvantage to specific groups of applicants.

Azure Machine Learning includes the capability to interpret models and quantify the extent to which each feature of the data influences the model's prediction. This capability helps data scientists and developers identify and mitigate bias in the model.

**Reliability and safety**

AI systems should perform reliably and safely. For example, consider an AI-based software system for an autonomous vehicle; or a machine learning model that diagnoses patient symptoms and recommends prescriptions. Unreliability in these kinds of system can result in substantial risk to human life.

AI-based software application development must be subjected to rigorous testing and deployment management processes to ensure that they work as expected before release.

**Privacy and security**

AI systems should be secure and respect privacy. The machine learning models on which AI systems are based rely on large volumes of data, which may contain personal details that must be kept private. Even after the models are trained and the system is in production, it uses new data to make predictions or take action that may be subject to privacy or security concerns.

**Inclusiveness**

AI systems should empower everyone and engage people. AI should bring benefits to all parts of society, regardless of physical ability, gender, sexual orientation, ethnicity, or other factors.

**Transparency**

AI systems should be understandable. Users should be made fully aware of the purpose of the system, how it works, and what limitations may be expected.

**Accountability**

People should be accountable for AI systems. Designers and developers of AI-based solution should work within a framework of governance and organizational principles that ensure the solution meets ethical and legal standards that are clearly defined.

## What is Machine Learning? <a id="1023974"></a>

Machine Learning is the foundation for most AI solutions, and enables the creation of models that predict unknown values and infer insights from observed data.

So how do machines learn?

The answer is, from data. In today's world, we create huge volumes of data as we go about our everyday lives. From the text messages, emails, and social media posts we send to the photographs and videos we take on our phones, we generate massive amounts of information. More data still is created by millions of sensors in our homes, cars, cities, public transport infrastructure, and factories.

Data scientists can use all of that data to train machine learning models that can make predictions and inferences based on the relationships they find in the data.

For example, suppose an environmental conservation organization wants volunteers to identify and catalog different species of wildflower using a phone app. The following animation shows how machine learning can be used to enable this scenario.

![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024042-412674.gif)

1. A team of botanists and data scientists collects samples of wildflowers.
2. The team labels the samples with the correct species.
3. The labeled data is processed using an algorithm that finds relationships between the features of the samples and the labeled species.
4. The results of the algorithm are encapsulated in a model.
5. When new samples are found by volunteers, the model can identify the correct species label.

## Regression <a id="1023975"></a>

Regression is a form of machine learning that is used to predict a numeric label based on an item's features. For example, suppose Adventure Works Cycles is a business that rents cycles in a city. The business could use historic data to train a model that predicts daily rental demand in order to make sure sufficient staff and cycles are available.

![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024038-412670.png)

To do this, Adventure Works could create a machine learning model that takes information about a specific day \(the day of week, the expected weather conditions, and so on\) as an input, and produces the predicted number of rentals as an output.

Mathematically, you can think of machine learning as a way of defining a function \(let's call it **f**\) that operates on one or more features of something \(which we'll call **x**\) to calculate a predicted label \(**y**\) - like this:

**y = f\(x\)**

Regression is an example of a supervised machine learning technique in which you train a model using data that includes both the features and known values for the label, so that the model learns to fit the feature combinations to the label. Then, after training has been completed, you can use the trained model to predict labels for new items for which the label is unknown.

In the bicycle rental example, the details about a given day \(day of the week, weather, and so on\) are the features \(**x**\), the number of rentals for that day is the label \(**y**\), and the function \(**f**\) that calculates the number of rentals based on the information about the day is the machine learning model.

The specific operation that the **f** function performs on x to calculate y depends on a number of factors, including the type of model you're trying to create and the specific algorithm used to train the model.

## Classification <a id="1023976"></a>

![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024040-412672.png)

Classification is a form of machine learning that is used to predict which category, or class, an item belongs to. For example, a health clinic might use the characteristics of a patient \(such as age, weight, blood pressure, and so on\) to predict whether the patient is at risk of diabetes. In this case, the characteristics of the patient are the features, and the label is a classification of either **0** or **1**, representing non-diabetic or diabetic.

Like regression, classification is an example of a supervised machine learning technique in which you train a model using data that includes both the features and known values for the label, so that the model learns to fit the feature combinations to the label. Then, after training has been completed, you can use the trained model to predict labels for new items for which the label is unknown.

## Clustering <a id="1023977"></a>

Clustering is a form of machine learning that is used to group similar items into clusters based on their features. For example, a botanist might take measurements of plants, and group them based on similarities in their proportions.

![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024024-412656.png)

Clustering is an example of unsupervised machine learning, in which you train a model to separate items into clusters based purely on their characteristics, or features. There is no previously known cluster value \(or label\) from which to train the model.

## What is Azure Machine Learning? <a id="1023979"></a>

Microsoft Azure provides the **Azure Machine Learning** service - a cloud-based platform for creating, managing, and publishing machine learning models. Azure Machine Learning provides the following features and capabilities:

| Feature | Capability |
| :--- | :--- |
| Automated machine learning | This feature enables non-experts to quickly create an effective machine learning model from data. |
| Azure Machine Learning designer | A graphical interface enabling no-code development of machine learning solutions. |
| Data and compute management | Cloud-based data storage and compute resources that professional data scientists can use to run data experiment code at scale. |
| Pipelines | Data scientists, software engineers, and IT operations professionals can define pipelines to orchestrate model training, deployment, and management tasks. |

## Automated Machine Learning <a id="1023980"></a>

Automated Machine Learning in Azure Machine Learning provides the easiest way to train a machine learning model for regression or classification \(or forecasting, which is really just regression with a time-series element\). There's a visual interface for automated machine learning in the Azure Machine Learning studio web portal. You just need to supply the training data and select the required model type, and Azure ML does the rest.

Automated machine learning helps data scientists increase their efficiency by automating many of the time-consuming tasks associated with training models; and it enables them to use cloud-based compute resources that scale effectively to run multiple training experiments in parallel while incurring costs only when actually used.

## Azure Machine Learning designer <a id="1023981"></a>

![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024026-412658.png)

In Azure Machine Learning, multi-step workflows to prepare data, train models, and perform model management tasks are called pipelines. The designer tool in Azure Machine Learning studio enables you to create and run pipelines by using a drag & drop visual interface to connect modules that define the steps and data flow for the pipeline

## What is Computer Vision? <a id="1023986"></a>

Computer vision is one of the core areas of artificial intelligence \(AI\), and focuses on creating solutions that enable AI-enabled applications to “see” the world and make sense of it.

![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024021-412653.png)

Of course, computers don't have biological eyes that work the way ours do, but they are capable of processing images; either from a live camera feed or from digital photographs or videos. This ability to process images is the key to creating software that can emulate human visual perception.

To an AI application, an image is just an array of pixel values. These numeric values can be used as features to train machine learning models that make predictions about the image and its contents.

## Applications of Computer Vision <a id="1023987"></a>

Most computer vision solutions are based on machine learning models that can be applied to visual input from cameras, videos, or images.

The following table describes common applications of computer vision.

| Task | Description |
| :--- | :--- |
| Image classification | ![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024032-412664.png) Image classification involves training a machine learning model to classify images based on their contents. For example, in a traffic monitoring solution you might use an image classification model to classify images based on the type of vehicle they contain, such as taxis, buses, cyclists, and so on. |
| Object detection | ![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024033-412665.png) Object detection machine learning models are trained to classify individual objects within an image, and identify their location with a bounding box. For example, a traffic monitoring solution might use object detection to identify the location of different classes of vehicle. |
| Semantic segmentation | ![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024034-412666.png) Semantic segmentation is an advanced machine learning technique in which individual pixels in the image are classified according to the object to which they belong. For example, a traffic monitoring solution might overlay traffic images with “mask” layers to highlight different vehicles using specific colors. |
| Image analysis | ![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024035-412667.png) You can create solutions that combine machine learning models with advanced image analysis techniques to extract information from images, including "tags" that could help catalog the image or even descriptive captions that summarize the scene shown in the image. |
| Face detection, analysis, and recognition | ![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024036-412668.png) Face detection is a specialized form of object detection that locates human faces in an image. This can be combined with classification and facial geometry analysis techniques to infer details such as age, and emotional state; and even recognize individuals based on their facial features. |
| Optical character recognition \(OCR\) | ![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024037-412669.png) Optical character recognition is a technique used to detect and read text in images. You can use OCR to read text in photographs \(for example, road signs or store fronts\) or to extract information from scanned documents such as letters, invoices, or forms. |

## Cognitive Services <a id="1023990"></a>

![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024041-412673.png)

Azure provides a set of cognitive services that encapsulate common AI workloads – including computer vision tasks.

To use cognitive services, you must provision a resource in your Azure subscription. This can be a standalone, service-specific resource \(for example, a Computer Vision resource\) or a general Cognitive Services resource that encapsulates multiple services. Using a standalone service-specific resource enables you to manage costs and access to that service independently of other services you may be using, while a general Cognitive Services resource enables you to combine all of your AI services in a single Azure resource for centralized management.

Regardless of the type of resource you use, client applications will consume the services it provides by connecting to a REST endpoint \(an HTTPS address to which they can submit requests\) using a resource-specific key for authentication.

You'll see this pattern of resource access repeatedly in the hands-on activities later in this module.

## Image Analysis with the Computer Vision Service <a id="1023991"></a>

The Computer Vision cognitive service provides a pre-trained computer vision model that you can use to analyze images.

![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024031-412663.png)

The model can:

* Detect and locate over 10,000 classes of common object.
* Generate an automatic caption for the image and a set of relevant tags that relate to its contents. Tags can be useful if you need to index images and search for them using relevant keywords.
* Detect and analyze human faces.
* Enable content moderation by identifying images that contain adult, racy, or gory content.
* Detect and extract text in an image \(we'll explore this in more depth later in the module\).

The Computer Vision service and API in the Microsoft Azure Cognitive Services offerings, provides pre-built algorithms that can process images that you upload. The service can return specific information about the content of the image supplied.

**Using the Computer Vision service**

The first step towards using the Computer Vision service is to create a resource for it in your Azure subscription. You can use either of the following resource types:

* **Computer Vision**: A specific resource for the Computer Vision service. Use this resource type if you don't intend to use any other cognitive services, or if you want to track utilization and costs for your Computer Vision resource separately.
* **Cognitive Services**: A general cognitive services resource that includes Computer Vision along with many other cognitive services; such as Text Analytics, Translator Text, and others. Use this resource type if you plan to use multiple cognitive services and want to simplify administration and development.

Whichever type of resource you choose to create, it will provide two pieces of information that you will need to use it:

* A **key** that is used to authenticate client applications.
* An **endpoint** that provides the HTTP address at which your resource can be accessed.

**Note**: If you create a Cognitive Services resource, client applications use the same key and endpoint regardless of the specific service they are using.

Client applications can then submit images in REST requests to the endpoint, using the key for authentication, and retrieve analysis results from the response.

## Training Models with the Custom Vision Service <a id="1023992"></a>

The Custom Vision cognitive services enables you to train a custom model for either image classification or object detection.

**Image Classification**

Image classification is a machine learning technique in which the object being classified is an image, such as a photograph.

![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024022-412654.png)

As with any form of classification, creating an image classification solution involves training a model using a set of existing data for which the class is already known. In this case, the existing data consists of a set of categorized images, which you must upload to the Custom Vision service and tag with appropriate class labels. After training the model, you can publish it as a service for applications to use.

**Object Detection**

Object detection is a form of machine learning based computer vision in which a model is trained to recognize individual types of object in an image, and to identify their location in the image.

![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024023-412655.png)

Creating an object detection solution with Custom Vision consists of three main tasks. First you must use upload and tag images, then you can train the model, and finally you must publish the model so that client applications can use it to locate objects in images.

**Using the Custom Vision Service**

Creating an image classification or object detection solution with Custom Vision requires a resource in your Azure subscription. You can use the following types of resource:

* **Custom Vision**: A dedicated resource for the custom vision service, which can be either a training or a prediction resource.
* **Cognitive Services**: A general cognitive services resource that includes Custom Vision along with many other cognitive services. You can use this type of resource for training, prediction, or both.

The separation of training and prediction resources is useful when you want to track resource utilization for model training separately from client applications using the model to predict image classes. However, it can make development of an image classification solution a little confusing.

The simplest approach is to use a general Cognitive Services resource for both training and prediction. This means you only need to concern yourself with one endpoint \(the HTTP address at which your service is hosted\) and key \(a secret value used by client applications to authenticate themselves\).

If you choose to create a Custom Vision resource, you will be prompted to choose training, prediction, or both - and it's important to note that if you choose “both”, then **two** resources are created - one for training and one for prediction.

It's also possible to take a mix-and-match approach in which you use a dedicated Custom Vision resource for training, but deploy your model to a Cognitive Services resource for prediction. For this to work, the training and prediction resources must be created in the same region.

**Model training**

To train a custom vision model, you must upload images to your training resource and label them with the appropriate class labels or object bounding boxes. Then, you must train the model and evaluate the training results.

You can perform these tasks in the Custom Vision portal, or if you have the necessary coding experience you can use one of the Custom Vision service programming language-specific software development kits \(SDKs\).

One of the key considerations when using images for classification, is to ensure that you have sufficient images of the objects in question and those images should be of the object from many different angles.

**Using the model for prediction**

After you've trained the model, and you're satisfied with its performance, you can publish the model to your prediction resource. When you publish the model, you can assign it a name \(the default is "IterationX", where X is the number of times you have trained the model\).

To use you model, client application developers need the following information:

* **Project ID**: The unique ID of the Custom Vision project you created to train the model.
* **Model name**: The name you assigned to the model during publishing.
* **Prediction endpoint**: The HTTP address of the endpoints for the prediction resource to which you published the model \(**not** the training resource\).
* **Prediction key**: The authentication key for the prediction resource to which you published the model \(**not** the training resource\).

## Analyzing Faces with the Face Service <a id="1023993"></a>

Face detection and analysis is an area of artificial intelligence \(AI\) in which we use algorithms to locate and analyze human faces in images or video content.

**Face detection**

Face detection involves identifying regions of an image that contain a human face, typically by returning bounding box coordinates that form a rectangle around the face, like this:

![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024027-412659.png)

**Facial analysis**

Moving beyond simple face detection, some algorithms can also return other information, such as facial landmarks \(nose, eyes, eyebrows, lips, and others\).

![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024028-412660.png)

These facial landmarks can be used as features with which to train a machine learning model from which you can infer information about a person, such as their age or peceived emotional state, like this:

![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024029-412661.png)

**Facial recognition**

A further application of facial analysis is to train a machine learning model to identify known individuals from their facial features. This usage is more generally known as facial recognition, and involves using multiple images of each person you want to recognize to train a model so that it can detect those individuals in new images on which it wasn't trained.

![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024030-412662.png)

**Uses of face detection and analysis**

There are many applications for face detection, analysis, and recognition. For example,

* Security - facial recognition can be used in building security applications, and increasingly it is used in smart phones operating systems for unlocking devices.
* Social media - facial recognition can be used to automatically tag known friends in photographs.
* Intelligent monitoring - for example, an automobile might include a system that monitors the driver's face to determine if the driver is looking at the road, looking at a mobile device, or shows signs of tiredness.
* Advertising - analyzing faces in an image can help direct advertisements to an appropriate demographic audience.
* Missing persons - using public cameras systems, facial recognition can be used to identify if a missing person is in the image frame.
* Identity validation - useful at ports of entry kiosks where a person holds a special entry permit.

**Using the Face service**

To use the Face service, you must create one of the following types of resource in your Azure subscription:

* **Face**: A specific resource for the Face service. Use this resource type if you don't intend to use any other cognitive services, or if you want to track utilization and costs for the Face service separately.
* **Cognitive Services**: A general cognitive services resource that includes Computer Vision along with many other cognitive services; such as Computer Vision, Text Analytics, Translator Text, and others. Use this resource type if you plan to use multiple cognitive services and want to simplify administration and development.

Whichever type of resource you choose to create, it will provide two pieces of information that you will need to use it:

* A **key** that is used to authenticate client applications.
* An **endpoint** that provides the HTTP address at which your resource can be accessed.

**Note**: If you create a Cognitive Services resource, client applications use the same key and endpoint regardless of the specific service they are using.

**Tips for more accurate results**

There are some considerations that can help improve the accuracy of the detection in the images.

For still images, such as photographs, consider:

* image format - supported images are JPEG, PNG, GIF, and BMP
* file size - 4 MB or smaller
* face size range - from 36 x 36 up to 4096 x 4096. Smaller or larger faces will not be detected
* other issues - face detection can be impaired by extreme face angles, occlusion \(objects blocking the face such as sunglasses or a hand\). Best results are obtained when the faces are full-frontal or as near as possible to full-frontal

Improving detection when using video feeds can be accomplished by considering the following aspects:

* smoothing - if your video camera applies this effect, turn it off. The potential blur between frames tends to reduce clarity of the image in individual frames.
* shutter speed - faster shutter speeds improves clarity of the images in each frame because the motion is reduced.
* shutter angle - if your camera supports shutter angle, use a lower shutter angle to produce clearer frames, resulting in better clarity for recognition.

## Reading Text with the Computer Vision Service <a id="1023994"></a>

The ability for computer systems to process written or printed text is an area of artificial intelligence \(AI\) where computer vision intersects with natural language processing. You need computer vision capabilities to “read” the text, and then you need natural language processing capabilities to make sense of it.

The basic foundation of processing printed text is optical character recognition \(OCR\), in which a model can be trained to recognize individual shapes as letters, numerals, punctuation, or other elements of text. Much of the early work on implementing this kind of capability was performed by postal services to support automatic sorting of mail based on postal codes. Since then, the state-of-the-art for reading text has moved on, and it's now possible to build models that can detect printed or handwritten text in an image and read it line-by-line or even word-by-word.

**Uses of OCR**

The ability to recognize printed and handwritten text in images, is beneficial in many scenarios such as:

* note taking
* digitizing forms, such as medical records or historical documents
* scanning printed or handwritten checks for bank deposits

**Reading text with the Computer Vision service**

The ability to extract text from images is handled by the Computer Vision service, which also provides image analysis capabilities.

The first step towards using the Computer Vision service is to create a resource for it in your Azure subscription. You can use either of the following resource types:

* **Computer Vision**: A specific resource for the Computer Vision service. Use this resource type if you don't intend to use any other cognitive services, or if you want to track utilization and costs for your Computer Vision resource separately.
* **Cognitive Services**: A general cognitive services resource that includes Computer Vision along with many other cognitive services; such as Text Analytics, Translator Text, and others. Use this resource type if you plan to use multiple cognitive services and want to simplify administration and development.

Whichever type of resource you choose to create, it will provide two pieces of information that you will need to use it:

* A **key** that is used to authenticate client applications.
* An **endpoint** that provides the HTTP address at which your resource can be accessed.

**Note**: If you create a Cognitive Services resource, client applications use the same key and endpoint regardless of the specific service they are using.

**Use the Computer Vision service to read text**

Many times an image contains text. It can be typewritten text or handwritten. Some common examples are images with road signs, scanned documents that are in an image format such as JPEG or PNG file formats, or even just a picture taken of a white board that was used during a meeting.

The Computer Vision service provides two application programming interfaces \(APIs\) that you can use to read text in images: the **OCR** API and the **Read** API.

**Note**: There is a third option, the Recognize Text API, but that is being deprecated. As a result, we won't discuss the Recognize Text API in this module.

**The OCR API**

The OCR API is designed for quick extraction of small amounts of text in images. It operates synchronously to provide immediate results, and can recognize text in numerous languages.

When you use the OCR API to process an image, it returns a hierarchy of information that consists of:

* **Regions** in the image that contain text
* **Lines** of text in each region
* **Words** in each line of text

For each of these elements, the OCR API also returns bounding box coordinates that define a rectangle to indicate the location in the image where the region, line, or word appears.

**The Read API**

The OCR method can have issues with false positives when the image is considered text-dominate. The Read API uses the latest recognition models and is optimized for images that have a significant amount of text or has considerable visual noise.

The Read API is a better option for scanned documents that have a lot of text. The Read API also has the ability to automatically determine the proper recognition model to use, taking into consideration lines of text and supporting images with printed text as well as recognizing handwriting.

Because the Read API can work with larger documents, it works asynchronously so as not to block your application while it is reading the content and returning results to your application. This means that to use the Read API, your application must use a three-step process:

1. Submit an image to the API, and retrieve an operation ID in response.
2. Use the operation ID to check on the status of the image analysis operation, and wait until it has completed.
3. Retrieve the results of the operation.

The results from the Read API are arranged into the following hierarchy:

* **Pages** - One for each page of text, including information about the page size and orientation.
* **Lines** - The lines of text on a page.
* **Words** - The words in a line of text.

Each line and word includes bounding box coordinates indicating its position on the page.

**Note**: If you are using the free-tier subscription, only the first two pages of a PDF or TIFF documents will be processed. Paid subscriptions support up to 200 pages and a maximum of 300 lines per page.

## Analyzing Forms with the Form Recognizer Service <a id="1023995"></a>

A common problem in many organizations is the need to process receipt or invoice data. For example, a company might require expense claims to be submitted electronically with scanned receipts, or invoices might need to be digitized and routed to the correct accounts department.

It's relatively easy to scan receipts to create digital images or PDF documents, and it's possible to use optical character recognition \(OCR\) technologies to extract the text contents from the digitized documents. However, typically someone still needs to review the extracted text to make sense of the information it contains.

For example, consider the following receipt.

![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024025-412657.jpg)

The receipt contains information that might be required for an expense claim, including:

* The name, address, and telephone number of the merchant.
* The date and time of the purchase.
* The quantity and price of each item purchased.
* The subtotal, tax, and total amounts.

Increasingly, organizations with large volumes of receipts and invoices to process are looking for artificial intelligence \(AI\) solutions that can not only extract the text data from receipts, but also intelligently interpret the information they contain.

**The Form Recognizer service**

The **Form Recognizer** service in Azure provides intelligent form processing capabilities that you can use to automate the processing of data in documents such as forms, invoices, and receipts. It combines state-of-the-art optical character recognition \(OCR\) with predictive models that can interpret form data by:

* Matching field names to values.
* Processing tables of data.
* Identifying specific types of field, such as dates, telephone numbers, addresses, totals, and others.

Form Recognizer supports automated document processing through:

* **Custom models**, which enable you to extract what are known as key/value pairs and table data from forms. Custom models are trained using your own data, which helps to tailor this model to your specific forms. Staring with only five samples of your forms, you can train the custom model. After the first training exercise, you can evaluate the results and consider if you need to add more samples and re train.
* **A pre-built receipt model** that is provided out-of-the-box, and is trained to recognize and extract data from sales receipts.

To use the Form recognizer service, you must create a **Form Recognizer** resource in your Azure subscription. After the resource has been created, you can create client applications that use its **key** and **endpoint** to connect submit forms for analysis.

**Using the pre-built receipt model**

Currently the pre-built receipt model is designed to recognize common receipts, in English, that are common to the USA. Examples are receipts used at restaurants, retail locations, and gas stations. The model is able to extract key information from the receipt slip:

* time of transaction
* date of transaction
* merchant information
* taxes paid
* receipt totals
* other pertinent information that may be present on the receipt
* all text on the receipt is recognized and returned as well

**Note**: The Form Recognizer service is in preview, as of the time this content was authored, and as a result, features and usage details may change. You should refer to the [Form Recognizer documentation](https://docs.microsoft.com/azure/cognitive-services/form-recognizer) for the service, for up-to-date information.

## What is Natural Language Processing? <a id="1024000"></a>

Natural language processing \(NLP\) is the area of AI that deals with creating software that understands written and spoken language.

NLP enables you to create software that can:

* Analyze text documents to extract key phrases and recognize entities \(such as places, dates, or people\).
* Perform sentiment analysis to determine how positive or negative the language used in a document is.
* Interpret spoken language, and synthesize speech responses.
* Automatically translate spoken or written phrases between languages.
* Interpret commands and determine appropriate actions.

## Natural Language Processing in Azure <a id="1024001"></a>

In Microsoft Azure, you can use the following cognitive services to build natural language processing solutions:

| Service | Capabilities |
| :--- | :--- |
| **Text Analytics** | Use this service to analyze text documents and extract key phrases, detect entities \(such as places, dates, and people\), and evaluate sentiment \(how positive or negative a document is\). |
| **Translator Text** | Use this service to translate text between more than 60 languages. |
| **Speech** | Use this service to recognize and synthesize speech, and to translate spoken languages. |
| **Language Understanding** | Use this service to train a language model that can understand spoken or text-based commands. |

## Text Analytics <a id="1024004"></a>

The Text Analytics service is a part of the Azure Cognitive Services offerings that can perform advanced natural language processing over raw text.

To use the Text Analytics service in an application, you must provision an appropriate resource in your Azure subscription. You can choose to provision either of the following types of resource:

* A **Text Analytics** resource - choose this resource type if you only plan to use the Text Analytics service, or if you want to manage access and billing for the resource separately from other services.
* A **Cognitive Services** resource - choose this resource type if you plan to use the Text Analytics service in combination with other cognitive services, and you want to manage access and billing for these services together.

**Language detection**

You can use the language detection capability of the Text Analytics service to identify the language in which text is written. You can submit multiple documents at a time for analysis. For each document submitted to it, the service will detect:

* The language name \(for example “English”\).
* The ISO 6391 language code \(for example, “en”\).
* A score indicating a level of confidence in the language detection.

For example, consider a scenario where you own and operate a restaurant where customers can complete surveys and provide feedback on the food, the service, staff, and so on. Suppose you have received the following reviews from customers:

**Review 1**: “A fantastic place for lunch. The soup was delicious.”

**Review 2**: “Comida maravillosa y gran servicio.”

**Review 3**: “The croque monsieur avec frites was terrific. Bon appetit!”

You can use the Text Analytics service to detect the language for each of these reviews; and it might respond with the following results:

| Document | Language Name | ISO 6391 Code | Score |
| :--- | :--- | :--- | :--- |
| Review 1 | English | en | 1.0 |
| Review 2 | Spanish | es | 1.0 |
| Review 3 | English | en | 0.9 |

Notice that the language detected for review 3 is English, despite the text containing a mix of English and French. The language detection service will focus on the **predominant** language in the text. The service uses an algorithm to determine the predominant language, such as length of phrases or total amount of text for the language compared to other languages in the text. The predominant language will be the value returned, along with the language code. The confidence score may be less than 1 as a result of the mixed language text.

**Ambiguous or mixed language content**

There may be text that is ambiguous in nature, or that has mixed language content. These situations can present a challenge to the service. An ambiguous content example would be a case where the document contains limited text, or only punctuation. For example, using the service to analyze the text ":-\)", results in a value of **unknown** for the language name and the language identifier, and a score of **NaN** \(which is used to indicate not a number\).

**Sentiment analysis**

The Text Analytics service can evaluate text and return sentiment scores and labels for each sentence. This capability is useful for detecting positive and negative sentiment in social media, customer reviews, discussion forums and more.

Using the pre-built machine learning classification model, the service evaluates the text and returns a sentiment score in the range of 0 to 1, with values closer to 1 being a positive sentiment. Scores that are close to the middle of the range \(0.5\) are considered neutral or indeterminate.

For example, the following two restaurant reviews could be analyzed for sentiment:

“We had dinner at this restaurant last night and the first thing I noticed was how courteous the staff was. We were greeted in a friendly manner and taken to our table right away. The table was clean, the chairs were comfortable, and the food was amazing.”

and

“Our dining experience at this restaurant was one of the worst I've ever had. The service was slow, and the food was awful. I'll never eat at this establishment again.”

The sentiment score for the first review might be around 0.9, indicating a positive sentiment; while the score for the second review might be closer to 0.1, indicating a negative sentiment.

**Indeterminate sentiment**

A score of 0.5 might indicate that the sentiment of the text is indeterminate, and could result from text that does not have sufficient context to discern a sentiment or insufficient phrasing. For example, a list of words in a sentence that has no structure, could result in an indeterminate score. Another example where a score may be 0.5 is in the case where the wrong language code was used. A language code \(such as “en” for English, or "fr" for French\) is used to inform the service which language the text is in. If you pass text in French but tell the service the language code is **en** for English, the service will return a score of precisely 0.5.

**Key phrase extraction**

Key phrase extraction is the concept of evaluating the text of a document, or documents, and then identifying the main points around the context or context of the document\(s\). Consider the restaurant scenario discussed previously. Depending on the volume of surveys that you have collected, it can take a long time to read through the reviews. Instead, you can use the key phrase extraction capabilities of the Text Analytics service and let the AI algorithms do the work for you.

You might receive a review such as:

“We had dinner here for a birthday celebration and had a fantastic experience. We were greeted by a friendly hostess and taken to our table right away. The ambiance was relaxed, the food was amazing, and service was terrific. If you like great food and attentive service, you should try this place.”

Key phrase extraction can provide some context to this review by extracting the following phrases:

* attentive service
* great food
* birthday celebration
* fantastic experience
* table
* friendly hostess
* dinner
* ambiance
* place

Not only can you use sentiment analysis to determine that this review is positive, you can use the key phrases to identify important elements of the review.

**Entity recognition**

You can provide the Text Analytics service with unstructured text and it will return a list of entities in the text that it recognizes. The service can also provide links to more information about that entity on the web. An entity is essentially an item of a particular type or a category; and in some cases, subtype, such as those as shown in the following table.

| Type | SubType | Example |
| :--- | :--- | :--- |
| Person |  | “Bill Gates”, "John" |
| Location |  | “Paris”, "New York" |
| Organization |  | “Microsoft” |
| Quantity | Number | "6" or “six” |
| Quantity | Percentage | "25%" or “fifty percent” |
| Quantity | Ordinal | "1st" or “first” |
| Quantity | Age | "90 day old" or “30 years old” |
| Quantity | Currency | "10.99" |
| Quantity | Dimension | “10 miles”, "40 cm" |
| Quantity | Temperature | “45 degrees” |
| DateTime |  | "6:30PM February 4, 2012" |
| DateTime | Date | “May 2nd, 2017” or "05/02/2017" |
| DateTime | Time | “8am” or "8:00" |
| DateTime | DateRange | “May 2nd to May 5th” |
| DateTime | TimeRange | "6pm to 7pm" |
| DateTime | Duration | “1 minute and 45 seconds” |
| DateTime | Set | "every Tuesday" |
| URL |  | "[https://www.bing.com](https://www.bing.com)" |
| Email |  | "support@microsoft.com" |
| US-based Phone Number |  | "\(312\) 555-0176" |
| IP Address |  | “10.0.1.125” |

The service also supports entity linking to help disambiguate entities by linking to a specific reference. For recognized entities, the service returns a URL for a relevant Wikipedia article.

For example, suppose you use the Text Analytics service to detect entities in the following restaurant review extract:

“I ate at the restaurant in Seattle last week.”

| Entity | Type | SubType | Wikipedia URL |
| :--- | :--- | :--- | :--- |
| Seattle | Location |  | [https://en.wikipedia.org/wiki/Seattle](https://en.wikipedia.org/wiki/Seattle) |
| last week | DateTime | DateRange |  |

## Speech Recognition and Synthesis <a id="1024005"></a>

Increasingly, we expect artificial intelligence \(AI\) solutions to accept vocal commands and provide spoken responses. Consider the growing number of home and auto systems that you can control by speaking to them - issuing commands such as “turn off the lights”, and soliciting verbal answers to questions such as "will it rain today?"

To enable this kind of interaction, the AI system must support two capabilities:

* **Speech recognition** - the ability to detect and interpret spoken input.
* **Speech synthesis** - the ability to generate spoken output.

Microsoft Azure offers both speech recognition and speech synthesis capabilities through the **Speech** cognitive service, which includes the following application programming interfaces \(APIs\):

* The **Speech-to-Text** API
* The **Text-to-Speech** API

**Using the Speech service**

To use the Speech service in an application, you must provision an appropriate resource in your Azure subscription. You can choose to provision either of the following types of resource:

* A **Speech** resource - choose this resource type if you only plan to use the Speech service, or if you want to manage access and billing for the resource separately from other services.
* A **Cognitive Services** resource - choose this resource type if you plan to use the Speech service in combination with other cognitive services, and you want to manage access and billing for these services together.

**The speech-to-text API**

You can use the speech-to-text API to perform real-time or batch transcription of audio into a text format. The audio source for transcription can be a real-time audio stream from a microphone or an audio file.

The model that is used by the speech-to-text API, is based on the Universal Language Model that was trained by Microsoft. The data for the model is Microsoft-owned and deployed to Microsoft Azure. The model is optimized for two scenarios, conversational and dictation. You can also create and train your own custom models including acoustics, language, and pronunciation if the pre-built models from Microsoft do not provide what you need.

**Real-time transcription**

Real-time speech-to-text allows you to transcribe text in audio streams. You can use real-time transcription for presentations, demos, or any other scenario where a person is speaking.

In order for real-time transcription to work, your application will need to be listening for incoming audio from a microphone, or other audio input source such as an audio file. Your application code streams the audio to the service, which returns the transcribed text.

**Batch transcription**

Not all speech-to-text scenarios are real time. You may have audio recordings stored on a file share, a remote server, or even on Azure storage. You can point to audio files with a shared access signature \(SAS\) URI and asynchronously receive transcription results.

Batch transcription should be run in an asynchronous manner because the batch jobs are scheduled on a best-effort basis. Normally a job will start executing within minutes of the request but there is no estimate for when a job changes into the running state.

**The text-to-speech API**

The text-to-speech API enables you to convert text input to audible speech, which can either be played directly through a computer speaker or written to an audio file.

**Speech synthesis voices**

When you use the text-to-speech API, you can specify the voice to be used to vocalize the text. This capability offers you the flexibility to personalize your speech synthesis solution and give it a specific character.

The service includes multiple pre-defined voices with support for multiple languages and regional pronunciation, including standard voices as well as neural voices that leverage neural networks to overcome common limitations in speech synthesis with regard to intonation, resulting in a more natural sounding voice. You can also develop custom voices and use them with the text-to-speech API

**Supported Languages**

Both the speech-to-text and text-to-speech APIs support a variety of languages. Use the links below to find details about the supported languages:

* [Speech-to-text languages](https://docs.microsoft.com/azure/cognitive-services/speech-service/language-support#speech-to-text).
* [Text-to-speech languages](https://docs.microsoft.com/azure/cognitive-services/speech-service/language-support#text-to-speech).

## Translation <a id="1024006"></a>

As organizations and individuals increasingly need to collaborate with people in other cultures and geographic locations, the removal of language barriers has become a significant problem.

One solution is to find bilingual, or even multilingual, people to translate between languages. However the scarcity of such skills, and the number of possible language combinations can make this approach difficult to scale. Increasingly, automated translation, sometimes known as machine translation, is being employed to solve this problem.

**Literal and semantic translation**

Early attempts at machine translation applied literal translations. A literal translation is where each word is translated to the corresponding word in the target language. This approach presents some issues. For one case, there may not be an equivalent word in the target language. Another case is where literal translation can change the meaning of the phrase or not get the context correct.

For example, the French phrase “éteindre la lumière” can be translated to English as "turn off the light". However, in French you might also say “fermer la lumiere” to mean the same thing. The French verb fermer literally means to "close", so a literal translation based only on the words would indicate, in English, "close the light"; which for the average English speaker, doesn't really make sense. To be useful, a translation service should take into account the semantic context and return an English translation of "turn off the light".

Artificial intelligence systems must be able to understand, not only the words, but also the semantic context in which they are used. In this way, the service can return a more accurate translation of the input phrase or phrases. The grammar rules, formal versus informal, and colloquialisms all need to be considered.

**Text and speech translation**

Text translation can be used to translate documents from one language to another, translate email communications that come from foreign governments, and even provide the ability to translate web pages on the Internet. Many times you will see a Translate option for posts on social media sites, or the Bing search engine can offer to translate entire web pages that are turned in search results.

Speech translation is used to translate between spoken languages, sometimes directly \(speech-to-speech translation\) and sometimes by translating to an intermediary text format \(speech-to-text translation\).

**Azure services for translation**

Microsoft Azure provides cognitive services that support translation. Specifically, you can use the following services:

* The **Translator Text** service, which supports text-to-text translation.
* The **Speech** service, which enables speech-to-text and speech-to-speech translation.

Before you can use the Translator Text or Speech services, you must provision appropriate resources in your Azure subscription.

There are dedicated **Translator Text** and **Speech** resource types for these services, which you can use if you want to manage access and billing for each service individually.

Alternatively, you can create a **Cognitive Services** resource that provides access to both services through a single Azure resource, consolidating billing and enabling applications to access both services through a single endpoint and authentication key.

**Text translation with the Translator Text service**

The Translator Text service is easy to integrate in your applications, websites, tools, and solutions. The service uses a Neural Machine Translation \(NMT\) model for translation, which analyzes the semantic context of the text and renders a more accurate and complete translation as a result.

**Translator Text service language support**

The Text Translator service supports text-to-text translation between [more than 60 languages](https://docs.microsoft.com/azure/cognitive-services/translator/languages). When using the service, you must specify the language you are translating **from** and the language you are translating **to** using ISO 639-1 language codes, such as en for English, fr for French, and zh for Chinese. Alternatively, you can specify cultural variants of languages by extending the language code with the appropriate 3166-1 cultural code - for example, en-US for US English, en-GB for British English, or fr-CA for Canadian French.

When using the Text Translator service, you can specify one **from** language with multiple **to** languages, enabling you to simultaneously translate a source document into multiple languages.

**Optional Configurations**

The Translator Text API offers some optional configuration to help you fine-tune the results that are returned, including:

* **Profanity filtering**. Without any configuration, the service will translate the input text, without filtering out profanity. Profanity levels are typically culture-specific but you can control profanity translation by either marking the translated text as profane or by omitting it in the results.
* **Selective translation**. You can tag content so that it isn't translated. For example, you may want to tag code, a brand name, or a word/phrase that doesn't make sense when localized.

**Speech translation with the Speech service**

The Speech service includes the following application programming interfaces \(APIs\):

* **Speech-to-text** - used to transcribe speech from an audio source to text format.
* **Text-to-speech** - used to generate spoken audio from a text source.
* **Speech Translation** - used to translate speech in one language to text or speech in another.

You can use the **Speech Translation** API to translate spoken audio from a streaming source, such as a microphone or audio file, and return the translation as text or an audio stream. This enables scenarios such as real-time closed captioning for a speech or simultaneous two-way translation of a spoken conversation.

**Speech service language support**

As with the Translator Text service, you can specify one source language and one or more target languages to which the source should be translated. You can translate speech into [over 60 languages](https://docs.microsoft.com/azure/cognitive-services/speech-service/language-support#speech-translation).

The source language must be specified using the extended language and culture code format, such as es-US for American Spanish. This requirement helps ensure that the source is understood properly, allowing for localized pronunciation and linguistic idioms.

The target languages must be specified using a two-character language code, such as en for English or de for German.

## Language Understanding <a id="1024007"></a>

As artificial intelligence \(AI\) grows ever more sophisticated, conversational interactions with applications and digital assistants is becoming more and more common, and in specific scenarios can result in human-like interactions with AI agents. Common scenarios for this kind of solution include customer support applications, reservation systems, and home automation among others.

To enable these kinds of conversational solution, computers need not only to be able to accept language as input \(either in text or audio format\), but also to be able to interpret the semantic meaning of the input - in other words, understand what is being said.

On Microsoft Azure, language understanding is supported through the **Language Understanding** service. To work with the **Language Understanding** service, you need to take into account three core concepts: utterances, entities, and intents.

**Utterances**

An utterance is an example of something a user might say, and which your application must interpret. For example, when using a home automation system, a user might use the following utterances:

“Switch the fan on.”

“Turn on the light.”

**Entities**

An entity is an item to which an utterance refers. For example, **fan** and **light** in the following utterances:

“Switch the **fan** on.”

“Turn on the **light**.”

You can think of the **fan** and **light** entities as being specific instances of a general **device** entity.

**Intents**

An intent represents the purpose, or goal, expressed in a user's utterance. For example, for both of the previously considered utterances, the intent is to turn a device on; so in your Language Understanding application, you might define a **TurnOn** intent that is related to these utterances.

A Language Understanding application defines a model consisting of intents and entities. Utterances are used to train the model to identify the most likely intent and the entities to which it should be applied based on a given input. The home assistant application we've been considering might include multiple intents, like the following examples:

| Intent | Related Utterances | Entities |
| :--- | :--- | :--- |
| Greeting | “Hello” |  |
|  | "Hi" |  |
|  | “Hey” |  |
|  | "Good morning" |  |
| TurnOn | “Switch the fan on” | fan \(device\) |
|  | "Turn the light on" | light \(device\) |
|  | “Turn on the light” | light \(device\) |
| TurnOff | "Switch the fan off" | fan \(device\) |
|  | “Turn the light off” | light \(device\) |
|  | "Turn off the light" | light \(device\) |
| CheckWeather | “What is the weather for today?” | today \(datetime\) |
|  | "Give me the weather forecast" |  |
|  | “What is the forecast for Paris?” | Paris \(location\) |
|  | "What will the weather be like in Seattle tomorrow?" | Seattle \(location\), tomorrow \(datetime\) |
| None | “What is the meaning of life?” |  |
|  | "Is this thing on?" |  |

In this table there are numerous utterances used for each of the intents. The intent should be a concise way of grouping the utterance tasks. Of special interest is the **None** intent. You should consider always using the None intent to help handle utterances that do not map any of the utterances you have entered. The None intent is considered a fallback, and is typically used to provide a generic response to users when their requests don't match any other intent.

**Tip** The **None** intent is created by default, and can't be deleted or renamed. Fill it with utterances that are outside of your domain.

After defining the entities and intents with sample utterances in your application, you can train a language model to predict intents and entities from user input - even if it doesn't match the sample utterances exactly. You can then use the model from a client application to retrieve predictions and respond appropriately.

**Creating a Language Understanding application**

Creating a language understanding application consists of two main tasks. First you must define entities, intents, and utterances with which to train the language model - referred to as authoring the model. Then you must publish the model so that client applications can use it for intent and entity prediction based on user input.

For each of the authoring and prediction tasks, you need a resource in your Azure subscription. You can use the following types of resource:

* **Language Understanding**: A dedicated resource for Language Understanding, which can be either an authoring or a prediction resource.
* **Cognitive Services**: A general cognitive services resource that includes Language Understanding along with many other cognitive services. You can only use this type of resource for prediction.

The separation of authoring and prediction resources is useful when you want to track resource utilization for language model training separately from client applications using the model to generate predictions. However, it can make development of a language understanding solution a little confusing.

If you choose to create a Language Understanding resource, you will be prompted to choose authoring, prediction, or both - and it's important to note that if you choose “both”, then **two** resources are created - one for authoring and one for prediction.

Alternatively, you can use a dedicated Language Understanding resource for authoring, but deploy your model to a generic Cognitive Services resource for prediction. When your client application uses other cognitive services in addition to Language Understanding, this approach enables you to manage access to all of the cognitive services being used, including the Language Understanding prediction service, through a single endpoint and key.

**Authoring**

After you've created an authoring resource, you can use it to author and train a Language Understanding application by defining the entities and intents that your application will predict as well as utterances for each intent that can be used to train the predictive model.

Language Understanding provides a comprehensive collection of prebuilt domains that include pre-defined intents and entities for common scenarios; which you can use as a starting point for your model. You can also create your own entities and intents.

When you create entities and intents, you can do so in any order. You can create an intent, and select words in the sample utterances you define for it to create entities for them; or you can create the entities ahead of time and then map them to words in utterances as you're creating the intents.

You can write code to define the elements of your model, but in most cases it's easiest to author your model using the Language Understanding portal - a web-based interface for creating and managing Language Understanding applications.

**Creating intents**

Define intents based on actions a user would want to perform with your application. For each intent, you should include a variety of utterances that provide examples of how a user might express the intent.

If an intent can be applied to multiple entities, be sure to include sample utterances for each potential entity; and ensure that each entity is identified in the utterance.

**Creating entities**

There are four types of entities:

* **Machine-Learned**: Entities that are learned by your model during training from context in the sample utterances you provide.
* **List**: Entities that are defined as a hierarchy of lists and sublists. For example, a **device** list might include sublists for **light** and **fan**. For each list entry, you can specify synonyms, such as **lamp** for **light**.
* **RegEx**: Entities that are defined as a regular expression that describes a pattern - for example, you might define a pattern like **\[0-9\]{3}-\[0-9\]{3}-\[0-9\]{4}** for telephone numbers of the form **555-123-4567**.
* **Pattern.any**: Entities that are used with patterns to define complex entities that may be hard to extract from sample utterances.

**Training the model**

After you have defined the intents and entities in your model, and included a suitable set of sample utterances; the next step is to train the model. Training is the process of using your sample utterances to teach your model to match natural language expressions that a user might say to probable intents and entities.

After training the model, you can test it by submitting text and reviewing the predicted intents. Training and testing is an iterative process. After you train your model, you test it with sample utterances to see if the intents and entities are recognized correctly. If they're not, make updates, retrain, and test again.

**Predicting**

When you are satisfied with the results from the training and testing, you can publish your Language Understanding application to a prediction resource for consumption.

Client applications can use the model by connecting to the endpoint for the prediction resource, specifying the appropriate authentication key; and submit user input to get predicted intents and entities. The predictions are returned to the client application, which can then take appropriate action based on the predicted intent.

## What is Conversational AI? <a id="1024012"></a>

In today's connected world, people use a variety of technologies to communicate. For example:

* Voice calls
* Messaging services
* Online chat applications
* Email
* Social media platforms
* Collaborative workplace tools

We've become so used to ubiquitous connectivity, that we expect the organizations we deal with to be easily contactable and immediately responsive through the channels we already use. Additionally, we expect these organizations to engage with us individually, and be able to answer complex questions at a personal level.

**Conversational AI**

While many organizations publish support information and answers to frequently asked questions \(FAQs\) that can be accessed through a web browser or dedicated app. The complexity of the systems and services they offer means that answers to specific questions are hard to find. Often, these organizations find their support personnel being overloaded with requests for help through phone calls, email, text messages, social media, and other channels.

Increasingly, organizations are turning to artificial intelligence \(AI\) solutions that make use of AI agents, commonly known as bots to provide a first-line of automated support through the full range of channels that we use to communicate. Bots are designed to interact with users in a conversational manner, as shown in this example of a chat interface:

![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024043-412675.png)

**Note**: The example shown here is a chat interface, such as you might find on a web site; but bots can be designed to work across multiple channels, including email, social media platforms, and even voice calls. Regardless of the channel used, bots typically manage conversation flows using a combination of natural language and constrained option responses that guide the user to a resolution.

Conversations typically take the form of messages exchanged in turns; and one of the most common kinds of conversational exchange is a question followed by an answer. This pattern forms the basis for many user support bots, and can often be based on existing FAQ documentation. To implement this kind of solution, you need:

* A knowledge base of question and answer pairs - usually with some built-in natural language processing model to enable questions that can be phrased in multiple ways to be understood with the same semantic meaning.
* A bot service that provides an interface to the knowledge base through one or more channels.

## Responsible AI Guidelines for Bots <a id="1024013"></a>

When designing a bot, developers should consider the following guidelines:

1. Be transparent about what the bot can \(and can't\) do
2. Make it clear that the user is communicating with a bot
3. Enable the bot to seamlessly hand-off to a human if necessary
4. Ensure the bot respects cultural norms
5. Ensure the bot is reliable
6. Respect user privacy
7. Handle data securely
8. Ensure the bot meets accessibility standards
9. Assume accountability for the bot's actions

These guidelines are based on the Microsoft paper [Responsible bots: 10 guidelines for developers of conversational AI](https://www.microsoft.com/research/publication/responsible-bots/).

You can also use this [interactive guidance](https://aidemos.microsoft.com/responsible-conversational-ai/building-a-trustworthy-bot) to learn more about building a trustworthy bot.

## The QnA Maker Service <a id="1024016"></a>

You can easily create a user support bot solution on Microsoft Azure using a combination of two core technologies:

* **QnA Maker**. This cognitive service enables you to create and publish a knowledge base with built-in natural language processing capabilities.
* **Azure Bot Service**. This service provides a framework for developing, publishing, and managing bots on Azure.

**Creating a QnA Maker knowledge base**

The first challenge in creating a user support bot is to use the QnA Maker service to create a knowledge base. The service provides a dedicated QnA Maker portal web-based interface that you can use to create, train, publish, and manage knowledge bases.

**Note**: You can write code to create and manage knowledge bases using the QnA Maker REST API or SDK. However, in most scenarios it is easier to use the QnA Maker portal.

**Provision a QnA Maker Azure resource**

To create a knowledge base, you must first provision a **QnA Maker** resource in your Azure subscription. You can do this directly in the Azure portal before you start creating your knowledge base, or you can start developing your knowledge base in the QnA Maker portal and provision the resource when prompted.

**Define questions and answers**

After provisioning a QnA Maker resource, you can use the QnA Maker portal to create a knowledge base that consists of question-and-answer pairs. These questions and answers can be:

* Generated from an existing FAQ document or web page.
* Imported from a pre-defined chit-chat data source.
* Entered and edited manually.

In many cases, a knowledge base is created using a combination of all of these techniques; starting with a base dataset of questions and answers from an existing FAQ document, adding common conversational exchanges from a chit-chat source, and extending the knowledge base with additional manual entries.

Questions in the knowledge base can be assigned alternative phrasing to help consolidate questions with the same meaning. For example, you might include a question like:

What is your head office location?

You can anticipate different ways this question could be asked by adding an alternative phrasing such as:

Where is your head office?

**Train and test the knowledge base**

After creating a set of question-and-answer pairs, you must train your knowledge base. This process analyzes your literal questions and answers and applies a built-in natural language processing model to match appropriate answers to questions, even when they are not phrased exactly as specified in your question definitions.

After training, you can use the built-in test interface in the QnA Maker portal to test your knowledge base by submitting questions and reviewing the answers that are returned.

**Publish the knowledge base**

When you're satisfied with your trained knowledge base, you can publish it so that client applications can use it over its REST interface. To access the knowledge base, client applications require:

* The knowledge base ID
* The knowledge base endpoint
* The knowledge base authorization key

## Azure Bot Service <a id="1024017"></a>

After you've created and published a knowledge base, you can use Azure Bot Service deliver it to users through a bot.

**Create a bot for your knowledge base**

You can create a custom bot by using the Microsoft Bot Framework SDK to write code that controls conversation flow and integrates with your QnA Maker knowledge base. However, an easier approach is to use the automatic bot creation functionality of QnA Maker, which enables you create a bot for your published knowledge base and publish it as an Azure Bot Service application with just a few clicks.

**Extend and configure the bot**

After creating your bot, you can manage it in the Azure portal, where you can:

* Extend the bot's functionality by adding custom code.
* Test the bot in an interactive test interface.
* Configure logging, analytics, and integration with other services.

For simple updates, you can edit bot code directly in the Azure portal. However, for more comprehensive customization, you can download the source code and edit it locally; republishing the bot directly to Azure when you're ready.

**Connect channels**

When your bot is ready to be delivered to users, you can connect it to multiple channels; making it possible for users to interact with it through web chat, email, Microsoft Teams, and other common communication media.

![](https://www.skillpipe.com/api/2/content/1112c950-7ed9-5056-b53b-5b523cf0d817/3/OEBPS/Images/1024039-412671.png)

Users can submit questions to the bot through any of its channels, and receive an appropriate answer from the knowledge base on which the bot is based.

